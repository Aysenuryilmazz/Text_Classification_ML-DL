{"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, roc_curve, confusion_matrix, classification_report, roc_auc_score\n\n\n\n# display test scores and return result string and indexes of false samples\ndef display_test_scores(test, pred):\n    str_out = \"\"\n    str_out += (\"TEST SCORES\\n\")\n    str_out += (\"=====================\\n\")\n    \n    #print accuracy\n    accuracy = accuracy_score(test, pred)\n    str_out += (\"ACCURACY: {:.4f}\\n\".format(accuracy))\n    str_out += (\"\\n\")\n    str_out += (\"--------------------\\n\")\n    \n\n    #print confusion matrix\n    str_out += (\"CONFUSION MATRIX:\\n\")\n    conf_mat = confusion_matrix(test, pred)\n    str_out += (\"{}\".format(conf_mat))\n    str_out += (\"\\n\")\n    str_out += (\"\\n\")\n    str_out += (\"--------------------\\n\")\n    \n    #print FP, FN\n    str_out += (\"FALSE POSITIVES:\\n\")\n    fp = conf_mat[1][0]\n    pos_labels = conf_mat[1][0]+conf_mat[1][1]\n    str_out += (\"{} out of {} positive labels ({:.4f}%)\\n\".format(fp, pos_labels,fp/pos_labels))\n    str_out += (\"\\n\")\n    str_out += (\"------------------------------------\\n\")\n\n    str_out += (\"FALSE NEGATIVES:\\n\")\n    fn = conf_mat[0][1]\n    neg_labels = conf_mat[0][1]+conf_mat[0][0]\n    str_out += (\"{} out of {} negative labels ({:.4f}%)\\n\".format(fn, neg_labels, fn/neg_labels))\n    str_out += (\"\\n\")\n    str_out += (\"------------------------------------\\n\")\n\n    #print classification report\n    str_out += (\"PRECISION, RECALL, F1 scores:\\n\\n\")\n    str_out += (\"{}\".format(classification_report(test, pred)))\n    \n    false_indexes = np.where(test != pred)\n    return str_out, false_indexes, accuracy\n\n\n\ndef run_model(model, params, cv, X_train, X_test, y_train, y_test):\n    \n    # grid search for parameters\n    grid = GridSearchCV(estimator=model, param_grid=params, cv=cv, n_jobs=-1)\n    grid.fit(X_train, y_train)\n    \n\n    # print best scores\n    print(\"The best parameters are %s with a score of %0.4f\"\n          % (grid.best_params_, grid.best_score_))\n\n    # prediction results\n    y_pred = grid.predict(X_test)\n\n    # print accuracy metrics\n    results, false, test_acc = display_test_scores(y_test, y_pred)\n    print(results)\n    \n    return grid.best_estimator_, false, test_acc, grid.best_score_\n    \n    \n    \n    ","metadata":{"collapsed":false,"_kg_hide-input":false},"execution_count":0,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}